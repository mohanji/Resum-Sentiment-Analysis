{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "iAmT3LR7fCC7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis"
      ],
      "metadata": {
        "id": "wUv9L5EKx8hG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"Instagram wants to limit hashtag spam.\")"
      ],
      "metadata": {
        "id": "tEJoP3-Gvk7X",
        "outputId": "ca67de8e-dbf7-4f28-debe-1ead0ebad2c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.988932728767395}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-Shot Classification"
      ],
      "metadata": {
        "id": "fypjxcpRyAZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"zero-shot-classification\", model = 'facebook/bart-large-mnli')\n",
        "classifier(\n",
        "    [\"Inter Miami wins the MLS\", \"Match tonight betwee Chiefs vs. Patriots\", \"Michael Jordan plans to sell Charlotte Hornets\"],\n",
        "    candidate_labels=[\"soccer\", \"football\", \"basketball\"]\n",
        "    )"
      ],
      "metadata": {
        "id": "sBPOHEKNvlx5",
        "outputId": "165d60ff-eff4-4787-da70-2350568189cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'sequence': 'Inter Miami wins the MLS',\n",
              "  'labels': ['soccer', 'football', 'basketball'],\n",
              "  'scores': [0.9162040948867798, 0.07244189083576202, 0.011354007758200169]},\n",
              " {'sequence': 'Match tonight betwee Chiefs vs. Patriots',\n",
              "  'labels': ['football', 'basketball', 'soccer'],\n",
              "  'scores': [0.9281435608863831, 0.0391676239669323, 0.032688744366168976]},\n",
              " {'sequence': 'Michael Jordan plans to sell Charlotte Hornets',\n",
              "  'labels': ['basketball', 'football', 'soccer'],\n",
              "  'scores': [0.9859175682067871, 0.009983371943235397, 0.004099058918654919]}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generation"
      ],
      "metadata": {
        "id": "SePDjPtBzzIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\", temperature=0.8)\n",
        "generator(\"Once upon a time, in a land where the King Pineapple was\")"
      ],
      "metadata": {
        "id": "nWuRe3UfypG-",
        "outputId": "f689061b-f983-42f7-c532-89661bfdb0ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"Once upon a time, in a land where the King Pineapple was a common crop, the Queen of the North had lived in a small village. The Queen had always lived in a small village, and her daughter, who was also the daughter of the Queen, had lived in a larger village. The royal family would come to the Queen's village, and then the Queen would return to her castle and live there with her daughters. In the middle of the night, she would lay down on the royal bed and kiss the princess at least once, and then she would return to her castle to live there with her men. In the daytime, however, the Queen would be gone forever, and her mother would be alone. The reason for this disappearance, in the form of the Great Northern Passage and the Great Northern Passage, was the royal family had always wanted to take the place of the Queen. In the end, they took the place of the Queen, and went with their daughter to meet the King. At that time, the King was the only person on the island who had ever heard of the Great Northern Passage, and his return was in the past.\\n\\nAfter Queen Elizabeth's death, the royal family went to the Great Northern Passage, to seek out the Princess of England and put her there. The Princess of England had been in\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Name and Entity Recognition"
      ],
      "metadata": {
        "id": "Q0LGofIk3N7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner = pipeline(\"ner\", grouped_entities=True)\n",
        "ner(\"The man landed on the moon in 1969. Neil Armstrong was the first man to step on the Moon's surface. He was a NASA Astronaut.\")"
      ],
      "metadata": {
        "id": "OBT_iipG0Eir",
        "outputId": "e8fefe18-d4bd-4884-e9ab-94a1ebd5dabe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER',\n",
              "  'score': np.float32(0.99960065),\n",
              "  'word': 'Neil Armstrong',\n",
              "  'start': 36,\n",
              "  'end': 50},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': np.float32(0.82190216),\n",
              "  'word': 'Moon',\n",
              "  'start': 84,\n",
              "  'end': 88},\n",
              " {'entity_group': 'ORG',\n",
              "  'score': np.float32(0.9842771),\n",
              "  'word': 'NASA',\n",
              "  'start': 109,\n",
              "  'end': 113},\n",
              " {'entity_group': 'MISC',\n",
              "  'score': np.float32(0.8394754),\n",
              "  'word': 'As',\n",
              "  'start': 114,\n",
              "  'end': 116}]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarization"
      ],
      "metadata": {
        "id": "Va86TX9E5e5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(\"\"\"\n",
        "In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.[1] At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.\n",
        "\n",
        "Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM).[2] Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets.[3]\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "xlXIRSal3iHj",
        "outputId": "8ae965de-c70d-47ee-ab88-6617ec1d1ccf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': ' In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechanism . Transformerers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM)'}]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resume Sentiment and Tone"
      ],
      "metadata": {
        "id": "JA71SHKxYaWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Data Scientist with over 10 years of experience in data analysis, applying machine learning and advanced analytics to solve complex business challenges. Specialized in machine learning models, optimizating processes, and solving business problems with data, aligning model development with business outcomes. Proven ability to lead end-to-end projects, from data collection and wrangling to production-ready deployment, using cloud platforms, modern ML frameworks, and containerized pipelines. Expertise in predictive analytics, inferential analytics, and statistical modeling. Experienced in SQL, Python, Databricks.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "h-8nUevcYe6H"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 1. Load your desired tokenizer\n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# 2. Tokenize the text without padding or truncation\n",
        "# We return tensors or lists to slice them manually\n",
        "tokens = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"][0]\n",
        "\n",
        "# 3. Define chunk size (leaving room for 2 special tokens if needed)\n",
        "chunk_size = 508\n",
        "\n",
        "# 4. Create chunks\n",
        "chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
        "\n",
        "# 5. Convert back to strings or add special tokens for model input\n",
        "decoded_chunks = []\n",
        "for chunk in chunks:\n",
        "    # This adds [CLS] and [SEP] and converts back to a format the model likes\n",
        "    final_input = tokenizer.prepare_for_model(chunk.tolist(), add_special_tokens=True)\n",
        "    decoded_chunks.append(tokenizer.decode(final_input['input_ids']))\n",
        "\n",
        "\n",
        "# Initialize sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")"
      ],
      "metadata": {
        "id": "2LwtVJEW-ao7",
        "outputId": "19dbb93d-f3d6-4413-feb0-24b001914281",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment analysis\n",
        "sentiment = sentiment_pipeline(decoded_chunks)[0]\n",
        "print(f\"Sentiment: {sentiment['label']}\")\n",
        "print(f\"Confidence: {100*sentiment['score']:.2f}%\")"
      ],
      "metadata": {
        "id": "sDbTFhBgYsL4",
        "outputId": "c1f7e19f-e393-4d3d-e91c-caba0dbf9da7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: POSITIVE\n",
            "Confidence: 99.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorize tone\n",
        "tone_pipeline = pipeline(\"zero-shot-classification\", model = 'facebook/bart-large-mnli',\n",
        "                        candidate_labels=[\"Senior\", \"Junior\", \"Trainee\", \"Blue-collar\", \"White-collar\", \"Self-employed\"])\n",
        "tone = tone_pipeline(decoded_chunks)[0]\n",
        "\n",
        "print(f\"Tone: {tone['labels']}\")\n",
        "print(f\"Confidence: {100*tone['scores']}\")"
      ],
      "metadata": {
        "id": "bxwaQGPEY75J",
        "outputId": "3ddefa0e-b6f7-405a-8ed8-1782a718d72a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tone: ['Senior', 'Blue-collar', 'White-collar', 'Self-employed', 'Junior', 'Trainee']\n",
            "Confidence: [0.4091757535934448, 0.23886899650096893, 0.20030029118061066, 0.06276989728212357, 0.05929991230368614, 0.029585164040327072]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image classification"
      ],
      "metadata": {
        "id": "HyTqiQXiakIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_classifier = pipeline(\n",
        "    task=\"image-classification\", model=\"google/vit-base-patch16-224\"\n",
        ")\n",
        "result = image_classifier(\n",
        "    \"https://images.unsplash.com/photo-1689009480504-6420452a7e8e?q=80&w=687&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\"\n",
        ")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "ICcDTfsZZf5K",
        "outputId": "351aaac7-7b68-4e80-e14f-55fe31b18b18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'Yorkshire terrier', 'score': 0.9792122840881348}, {'label': 'Australian terrier', 'score': 0.00648861238732934}, {'label': 'silky terrier, Sydney silky', 'score': 0.00571345305070281}, {'label': 'Norfolk terrier', 'score': 0.0013639888493344188}, {'label': 'Norwich terrier', 'score': 0.0010306559270247817}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pAMpqEVkamDw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}